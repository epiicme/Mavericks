\chapter{Machine Learning for Speech}\label{ch:machine_learning}

\section{Historical Overview}
Markov Model, ETC...\\\\
Rise of Artificial Neural Network...\\\\
This brain modeling approach is a less technical way to develop machine learning solutions, but it is known to be more efficient and accurate than previous traditional approaches,
such as the Hidden Markov Model (HMM). 

\section{Artificial Neural Networks}
%---------------------------------------------------
\subsection{What is an Artificial Neural Network?}
Artificial Neural Networks are nothing but a computerized representation of the human brain. 
They have the ability to acquire and maintain knowledge (information based) and can be defined as a set of processing units, represented by artificial neurons,
interlinked by a lot of interconnections
(artificial synapses) \cite[p.~5]{Silva2016}.\\\\
The human brain learns from its experiences, creating new neural pathways. These interconneted chains of neurons are stronger than others and can be modulated, or changed,
following learning or during behavioral modifications.
The strongness of a neural connection is given by a weighted value, called synaptic weights.

\subsection{Artificial Neuron}
The Artificial neuron is the processing unit of an artificial neural network, which is simplified model of biological neuron.
This model was inspired by the analysis of how a cell membrane of a neuron generates and propagates electrical impulses(Hodgkin and Huxley 1952) \cite[p.~11]{Silva2016}.
The purpose of artificial neurons is to simulate the basic function of biological ones,
which are typically comprised of four parts:

\begin{enumerate}
	\item Dendrites: Accept inputs
	\item Soma(Cell body): Process the inputs
	\item Axon: Turn the processed inputs into outputs
	\item Synapses: The electrochemical connections between neurons 
\end{enumerate}

The artificial neurons used in artificial neural networks are nonlinear, usually providing continuous outputs,
and performing simple functions,
such as gathering signals available on their inputs,
assembling them according to their operational functions,
and producing a response considering their innate activation functions \cite[p.~11]{Silva2016}. 

\begin{figure}[h]
\centering
	\includegraphics[width=\textwidth]
	{machine_learning/00_Basic_Artificial_Neuron0.pdf}
	\caption{The artificial neuron.}
	\label{fig:AN}
\end{figure}

Each neuron from a network can be implemented as shown in Fig.
\ref{fig:AN}. The multiple input signals coming from the external environment are represented by the set
$\left\{i1,i2,i3,...,in \right\}$, analogous to the external electrical impulses gathered by the dendrites in the biological neuron.
\todo{re-write}

%---------------------------------------------------

\section{Types of Artificial Neural Networks}
%---------------------------------------------------
\subsection{Feedforward Network}
\subsection{Recurrent Neural Network}
\subsection{Modular Neural Network}
\subsection{Kohonen Self Organizing Maps}
\subsection{Convolutional Neural Network}
%---------------------------------------------------


\section{TensorFlow}
%---------------------------------------------------
TensorFlow provides multiple Application Programming Interfaces
(APIs) for machine learning. 
The lowest level API "TensorFlow Core" provides complete programming control \cite{tensorflow2015-whitepaper}. 
For those who require fine levels of control over their models,
TensorFlow Core is a well-suited tool for the job. There are higher level APIs that are built on top of TensorFlow Core.
These higher level APIs are typically easier to learn and use than Tensorflow Core \cite{tensorflow2015-whitepaper}.
In addition, the higher level APIs such as "tf.estimator" helps with managing data sets, estimators,
training and inferences (testing your trained network),
as well as, making repetitive tasks easier and more consistent
\cite{tensorflow2015-whitepaper}.\\

The subsection below will start with TensorFlow Core. 
In order to gain an understanding of the basic principles that TensorFlow has to offer, a model shall be made. 
In the subsection after that, the same model will be implemented with tf.estimator. 
Knowing TensorFlow Core principles will give us a great mental model of how things are working internally when we use the more compact higher level API.

\subsection{Tensors}
The central unit of data in TensorFlow is the tensor. 
A tensor consists of a set of primitive values shaped into an array of any number of dimensions. 
A tensor's rank is its number of dimensions. 
Here are some examples of tensors:

\begin{adjustbox}{width=\textwidth}
\begin{lstlisting}
3 # a rank 0 tensor; this is a scalar with shape []
[1., 2., 3.] # a rank 1 tensor; this is a vector with shape [3]
[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
\end{lstlisting} 
\end{adjustbox}

\todo{customize colors}

\subsection{TensorFlow Core}
Getting Started With TensorFlow is done by using the import statement. 
This gives Python access to all of TensorFlow's classes,
methods, and symbols and is called by the following statement:

\begin{lstlisting}
import tensorflow as tf
\end{lstlisting}

All the programs written with the help of TensorFlow core are built around computational graphs. 
They are a series of operations arranged into a graph and connected by nodes. 
To see the outcome of a such a program, 
firstly the computational graph needs to be created and after that it needs to be run. 
This provides a contrast to normal code written in python and to display this, 
the print function for a tensorflow constant is called bellow.

\begin{lstlisting}
node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)

Output:
Tensor("Const:0", shape=(), dtype=float32)
Tensor("Const_1:0", shape=(), dtype=float32) 
\end{lstlisting}

As seen above, printing an element does not show the value that it holds and it rather shows the technical details behind,
such as the element being a constant of type float32.
To investigate the contents of any element in tensorflow,
an object of type session needs to be defined.
When the new object is run, a new session is generated and the content of the element can be seen.
All code written is tensorflow core follows this basic principle.

\begin{lstlisting}
sess = tf.Session()
print(sess.run([node1, node2]))

Output:
[3.0]
\end{lstlisting}

Other basic elements in tensorflow are placeholders(tf.placeholder), 
they can be changed to accept external inputs and variables.
Variables allow the system to change its outputs while keeping the seame inputs, this allows the model to be trainable. 

\subsection{tf.train API}
The tf.train API holds optimizers that change each variable to minimize the loss function.
One of the simplest optimizers used to train neural networks is gradient descent.
Each variable is modified by the magnitude of the derivative of loss with respect to that variable.
Using a computer to gain these gradients is far less prone to error than doing it by hand.
Each of the gradients will point towards the minimum of the loss function and when solving for the gradient a step needs to be determined as the rate of change.
It is important to  have a small step as to not jump over the valley where the function takes its minimum value,
but in the beginning to reduce time and processing power a bigger step could be used.
One solution to this problem is to use a variable step input that changes as the gradients are determined.
TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients.
For simplicity, optimizers typically do this for us.
\subsection{tf.estimator API}
%---------------------------------------------------

\section{Deep neural network for speech}
%---------------------------------------------------
\subsection{Deep Feed Forward (DFF)}
The simplest of all neural networks, the feedforward neural network, moves information in one direction only.
Data moves from the input nodes to the output nodes,
passing through hidden nodes (if any).
The feedforward neural network has no cycles or loops in its network.
\todo{reference this part with
$https://www.allerin.com/blog/six-types-of-neural-networks$}

\subsection{Recurrent Neural Nerwork (RNN)}
The recurrent neural network, unlike the feedforward neural network, is a neural network that allows for a bi-directional flow of data. 
The network between the connected units forms a directed cycle.
Such a network allows for dynamic temporal behavior to be exhibited.
The recurrent neural network is capable of using its internal memory to process arbitrary sequence of inputs.
This neural network is a popular choice for tasks such as handwriting and speech recognition.
\todo{link this part with link above}

\subsubsection{Long/Short Term Memory (LSTM)}
%---------------------------------------------------
