\chapter{Machine Learning for Speech}\label{ch:machine_learning}



\section{TensorFlow}
TensorFlow provides multiple Application Programming Interfaces (APIs) for machine learning. The lowest level API "TensorFlow Core" provides complete programming control \cite{tensorflow2015-whitepaper}. For those who require fine levels of control over their models, TensorFlow Core is a well-suited tool for the job. There are higher level APIs that are built on top of TensorFlow Core. These higher level APIs are typically easier to learn and use than Tensorflow Core \cite{tensorflow2015-whitepaper}. In addition, the higher level APIs such as "tf.estimator" helps with managing data sets, estimators, training and inferences (testing your trained network), as well as, making repetitive tasks easier and more consistent \cite{tensorflow2015-whitepaper}.\\

The subsection below will start with TensorFlow Core. In order to gain an understanding of the basic principles that TensorFlow has to offer, a model shall be made. In the subsection after that, the same model will be implemented with tf.estimator. Knowing TensorFlow Core principles will give us a great mental model of how things are working internally when we use the more compact higher level API.

\subsection{Tensors}
The central unit of data in TensorFlow is the tensor. A tensor consists of a set of primitive values shaped into an array of any number of dimensions. A tensor's rank is its number of dimensions. Here are some examples of tensors:

\begin{verbatim}
3 # a rank 0 tensor; this is a scalar with shape []
[1., 2., 3.] # a rank 1 tensor; this is a vector with shape [3]
[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
\end{verbatim} \todo{customize colors}


\subsection{TensorFlow Core}
Getting Started With TensorFlow is done by using the import statement. This gives Python access to all of TensorFlow's classes, methods, and symbols and is called by the following statement:

\begin{lstlisting}
import tensorflow as tf
\end{lstlisting}

All the programs written with the help of TensorFlow core are built around computational graphs. They are a series of operations arranged into a graph and connected by nodes. To see the outcome of a such a program, firstly the computational graph needs to be created and after that it needs to be run. This provides a contrast to normal code written in python and to display this, the print function for a tensorflow constant is called bellow.
\begin{lstlisting}
node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32) 
\end{lstlisting}

As seen above, printing an element does not show the value that it holds and it rather shows the technical details behind, such as the element being a constant of type float32. To investigate the contents of any element in tensorflow, an object of type session needs to be defined. When the new object is run, a new session is generated and the content of the element can be seen. All code written is tensorflow core follows this basic principle.
 \begin{lstlisting}
 sess = tf.Session()
print(sess.run([node1, node2]))
[3.0]
\end{lstlisting}
Other basic elements in tensorflow are placeholders(tf.placeholder), they can be changed to accept external inputs and variables. Variables allow the system to change its outputs while keeping the seame inputs, this allows the model to be trainable. 

\subsection{tf.train API}
The tf.train API holds optimizers that change each variable to minimize the loss function. One of the simplest optimizers used to train neural networks is gradient descent. Each variable is modified by the magnitude of the derivative of loss with respect to that variable. Using a computer to gain these gradients is far less prone to error than doing it by hand. Each of the gradients will point towards the minimum of the loss function and when solving for the gradient a step needs to be determined as the rate of change. It is important to  have a small step as to not jump over the valley where the function takes its minimum value, but in the beginning to reduce time and processing power a bigger step could be used. One solution to this problem is to use a variable step input that changes as the gradients are determined. TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for us.
\subsection{tf.estimator API}

\section{Deep neural network for speech}

\subsection{Deep Feed Forward (DFF)}
The simplest of all neural networks, the feedforward neural network, moves information in one direction only. Data moves from the input nodes to the output nodes, passing through hidden nodes (if any). The feedforward neural network has no cycles or loops in its network.
\todo{reference this part with $https://www.allerin.com/blog/six-types-of-neural-networks$}

\subsection{Recurrent Neural Nerwork (RNN)}
The recurrent neural network, unlike the feedforward neural network, is a neural network that allows for a bi-directional flow of data. The network between the connected units forms a directed cycle. Such a network allows for dynamic temporal behavior to be exhibited. The recurrent neural network is capable of using its internal memory to process arbitrary sequence of inputs. This neural network is a popular choice for tasks such as handwriting and speech recognition.\todo{link this part with link above}

\subsubsection{Long/Short Term Memory (LSTM)}
